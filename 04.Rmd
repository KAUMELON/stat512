# Analysis of Variance (ANOVA)

Consider an example where three chemical fertilizers A, B, C are tested
on potted plants. Experimenter would like to identify, which among the
fertilizer among A, B and C gives the highest yield. Potted plants are
maintained in the same way; so that experimental conditions are
homogenous. Now after collecting yield data from the plants, you can
observe a variance *i*.*e*., the yield values are not same. Here, this
variation is caused by the treatment and experimental error. So, the
total observed variance = variance due to treatments + variance due to
error. So, in this example treatment and error can be considered as the
source of variation in the data. Basically, any experiment you perform
is a sample based on which you make generalization about the population.
Now, based on your experiment you want to test whether the means of
treatment A, B and C are significantly different considering that the
observed difference is not by chance taking in to account experimental
error variance.

Analysis of variance (ANOVA) is a statistical procedure used to analyze
the differences among means, where the observed total variance is
partitioned into components attributable to different sources of
variation. The logic behind is simple, if much of the variation comes
from the treatment, it is more likely that the mean of treatments is
different. This variation is compared with the experimental error
variance, larger the ratio of treatment variance to error variance, the
more likely that the groups have different means. The term \"analysis of
variance\" originates from how the analysis uses variances to determine
whether the means are different. ANOVA works by comparing the variance
of treatments (between group variance) to the error variance (within
group variance).

In short ANOVA is a statistical hypothesis test that determines whether
the means of at least two populations are different. ANOVA was developed
by the statistician Sir Ronald A. Fisher.

## Null hypothesis in ANOVA

Null hypothesis in ANOVA is that population means are equal, which is
denoted as H~0~: µ~1~ = µ~2~ =µ~3~ = ...., = µ~k~ . Alternate
hypothesis is that atleast a pair of treatment are not equal, H~1~: µ~i~ ≠ µ~j~, where *i* ≠ *j*; *i*, *j* = 1,2, ..., k. Where µ~1~, µ~2~, µ~3~, ..., µ~k~ are *k* population means

## Degrees of freedom

Before proceeding, it is important to understand the concept of degrees
of freedom. Degrees of freedom can be defined as the number of
independent observations that are free to vary.

Consider a simple example, you have 7 hats and you want to wear a
different hat every day of the week. On the first day, you can wear any
of the 7 hats. On the second day, you can choose from the 6 remaining
hats, on day 3 you can choose from 5 hats, and so on. When day 6 rolls
around, you still have a choice between 2 hats that you haven't worn yet
that week. But after you choose your hat for day 6, you have no choice
for the hat that you wear on Day 7. You must wear the one remaining hat.
You had 7-1 = 6 days of "hat" freedom---in which the hat you wore could
vary!

Degrees of freedom can also be defined as the number of independent
values, which were included into calculation of an estimate. An estimate
is a single number that expresses some property of a population from a
sample. It can be mean, median, standard deviation, or variance that is
calculated from a sample. And there are independent values (or
observations) that went into formula calculation. The quantity of these
values is called "degrees of freedom".

Consider three observations 6, x and 9. Here x is unknown and suppose we
know the mean is 6. Then we can say x is exactly equal to 3, because
mean is 6. Here two values are free to vary but the third value depends
on other two under the constraint that their mean should be 6. *i*.*e*.
if other two values are changed third value will also change.

Now consider the height of 7 students

164, 173, 158, 179, 168, 187, 167.

Mean: 170.85

We can find standard deviation using two formulas

SD= $\frac{\sum_{}^{}\left( x_{i} - \overline{x} \right)^{2}}{n}$, where
n is the number of observations

Here SD = 9

SD= $\frac{\sum_{}^{}\left( x_{i} - \overline{x} \right)^{2}}{n - 1}$,
where n-1 is the degrees of freedom

Here SD = 9.72

It is easy to notice that when we divide by degrees of freedom, we make
our estimate of standard deviation greater than if we were diving only
by sample size. But why do we need to make it greater? As we've already
calculated the mean, we don't have to use all the data in order to
estimate the standard deviation. It does not depend on each piece of
information, and the last observation does not contribute to the
standard deviation. So, if we don't delete this redundant data, then we
underestimate the standard deviation of population from sample data. So here using the degrees of freedom in the denominator provides an unbiased estimate of population standard deviation.

Degrees of freedom also define the probability distributions for the
test statistics of various hypothesis tests. For example, hypothesis
tests use the t-distribution, F-distribution, and the chi-square
distribution to determine statistical significance. Each of these
probability distributions is a family of distributions where the DF
define the shape. Hypothesis tests use these distributions to make
decisions on null hypothesis.  

## Mean squares  
The sum of squares is the sum of the square of difference of each
observation from mean. That is Sum of squares
=$\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2}$. Where, *i*
=1, 2, ..., *n*, mean $\overline{x} = \frac{\sum_{i = 1}^{n}x_{i}}{n}$.
Sum of squares generates a measure of variability. In One-way ANOVA we
calculate sum of squares of all observation to get measure of total
variability along with within group and between group sum of squares,
that gives idea on with in (error) and between group (between
treatments) variability respectively. Mean sum of squares or mean
squares are obtained by dividing sum of squares by corresponding degrees
of freedom. Mean square provides unbiassed estimate of variance. For
example, in ANOVA error mean square (MSE) given by
$\frac{\text{with in group sum of squares}}{\text{error degrees of freedom}}$
provides unbiased estimate of error variance  

## Test Statistic F

Any function of sample values is known as a statistic. For example,
sample mean, sample variance, sum of all sample values are statistic,
because these are all some functions of sample values. If a statistic is
used to test a hypothesis, then it is known as test statistic. Examples
of test statistic are t, F, χ^2^ etc. The test statistic used in ANOVA
is F.

F-statistic is the ratio of two variances and it was named after Sir
Ronald A. Fisher. It is proved that when null hypothesis H~0~: µ~1~ =
µ~2~ =µ~3~ = ...., = µ~k~ is true the ratio of mean square treatment and
mean square error follows F distribution. In general, if your calculated
F value in ANOVA is larger than your F critical value, you can reject
the null hypothesis.  

```{r fdist, echo=FALSE,fig.cap='Distribution F under different degrees of freedom',out.width="60%", fig.align='center'}
knitr::include_graphics(rep("images/fdist.png"))
```   

## Assumptions of ANOVA  
-   Observations under each treatment group is drawn randomly from a normally distributed population

-   All the populations from which observations are drawn have a common variance :- Homogeneity of variance

-   All the samples are drawn independently of each other

-   Treatment effects are additive

>As a result of above assumptions. The experimental errors are independent and identically distributed (iid) with a Normal distribution of mean 0 and variance σ^2^. In other word we can say $e_{i}$ $\sim iid\ N(0,\sigma^{2})$.


## Models under ANOVA  
Analysis of variance is based on a linear statistical model. This linear model may be either

-   Fixed effects model

-   Random effects model

-   Mixed effects model

### Fixed effects model

Fixed effects model is a statistical model in which the model parameters
are fixed or non-random quantities. In using fixed effect model in
agricultural experiments, we assume treatment effect are unknown
constants

Fixed effects model can be explained using an example. Consider an
experiment where experimenter wants to know whether three fields had any
impact on the yield of a particular strain of wheat. Observations where
taken from 12 plots from each fields (replication). The AOV (Analysis of
Variance) model is

$Y_{\text{ij}} = \mu + \tau_{i} + e_{\text{ij}}$

Where *i* = 1,2, ..., *t* and *j* =1,2, ..., *r*. Here in our example
*t* =3 and *r* =12. $Y_{\text{ij}}$ is the observed value of *i*^th^
field from *j*^th^ plot, $\tau_{i}$ is the effect of *i*^th^ treatment,
which is considered to be an unknown constant. $\mu$ is the general
effect which is common for all treatments. $e_{\text{ij}}$ is the error
effects, which are independently and normally distributed with mean 0
and constant variance σ^2^. In this model, field is a "fixed effect".
Along with $\mu$, the fixed parameters $\tau_{1}$,$\tau_{2}$ and
$\tau_{3}$ are the quantities of interest. Using this model,
experimenter can make decisions only on the treatment (field) tested.

### Random effects model

A random effects model, also called as a variance components model, is a
statistical model where the model parameters are random variables.
Consider the example above in of wheat fields, if a random effect model
is used then, fields are assumed to be randomly sampled from a
population of fields in that area and the AOV (Analysis of Variance)
model is

$Y_{\text{ij}} = \mu + \tau_{i} + e_{\text{ij}}$

Where *i* = 1,2, ..., *t* and *j* =1,2, ..., *r*. Here $\tau_{i}$ is
considered as a random variable,
$\tau_{i}\sim N(0,\ \sigma_{\tau}^{2})$; and
$e_{ij}\sim N(0,\ \sigma^{2})$. In this model, field is a "random
effect". The statistical model describes the whole ensemble of possible
repetitions of the experiment in the region from which the fields were
selected. Experimenter could make generalisations on all the fields in
that particular region based on his experiment. One important
consequence of random effects is that the responses ($Y_{\text{ij}}$\'s)
are no longer independent. The random $\tau_{i}$\'s induce correlations
among the responses. The responses jointly have a multivariate normal
distribution.

### Mixed effects model

A mixed model, mixed-effects model or mixed error-component model is a
statistical model containing both fixed effects and random effects.


## One-way ANOVA  

## Two-way ANOVA